{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import layers\n",
    "import random\n",
    "import joblib\n",
    "import pickle\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파일 경로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path='./save_model/'\n",
    "prediction_path='prediction_model/'\n",
    "stopwords_path='stopwords/'\n",
    "tokenizer_path='tokenizer_model/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN 구조 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(tf.keras.Model):\n",
    "    def __init__(self, **kargs):\n",
    "        super(CNNClassifier, self).__init__(name=kargs['model_name'])\n",
    "        self.embedding=layers.Embedding(input_dim=kargs['vocab_size'],\n",
    "                                       output_dim=kargs['embedding_size'])\n",
    "        self.conv_list=[layers.Conv1D(filters=kargs['num_filters'],\n",
    "                                     kernel_size=kernel_size,\n",
    "                                     padding='valid',\n",
    "                                     activation=tf.keras.activations.relu,\n",
    "                                     kernel_constraint=tf.keras.constraints.MaxNorm(max_value=3.))\n",
    "                       for kernel_size in [3,4,5]]\n",
    "        self.pooling=layers.GlobalMaxPooling1D()\n",
    "        self.dropout=layers.Dropout(kargs['dropout_rate'])\n",
    "        self.fc1=layers.Dense(units=kargs['hidden_dimension'],\n",
    "                             activation=tf.keras.activations.relu,\n",
    "                             kernel_constraint=tf.keras.constraints.MaxNorm(max_value=3.))\n",
    "        self.fc2=layers.Dense(units=kargs['output_dimension'],\n",
    "                             activation=tf.keras.activations.sigmoid,\n",
    "                             kernel_constraint=tf.keras.constraints.MaxNorm(max_value=3.))\n",
    "        \n",
    "    def call(self, x):\n",
    "        x=self.embedding(x)\n",
    "        x=self.dropout(x)\n",
    "        x=tf.concat([self.pooling(conv(x)) for conv in self.conv_list], axis=-1)\n",
    "        x=self.fc1(x)\n",
    "        x=self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 로드 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 로드\n",
    "def get_tokenizer():\n",
    "    with open(base_path+tokenizer_path+'tokenizer.pickle', 'rb') as f:\n",
    "        loaded_tokenizer=pickle.load(f)\n",
    "    return loaded_tokenizer\n",
    "\n",
    "# rfc 모델 로드\n",
    "def get_rfc_model():\n",
    "    loaded_rfc_model= joblib.load(base_path+prediction_path+'rfc_model.pkl')\n",
    "    return loaded_rfc_model\n",
    "\n",
    "# xgb 모델 로드\n",
    "def get_xgb_model():\n",
    "    loaded_model = pickle.load(open('D:/우편물류/구성원 추정/save_model/prediction_model/'+\"xgb_model.pickle\", \"rb\"))\n",
    "    return loaded_model\n",
    "\n",
    "# rnn 모델 구조 & 가중치 로드\n",
    "def get_rnn_model():\n",
    "    json_file = open(base_path+prediction_path+'rnn_model_structure.json', 'r')\n",
    "    loaded_rnn_structure = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_rnn_model = model_from_json(loaded_rnn_structure)\n",
    "    loaded_rnn_model.load_weights(base_path+prediction_path+\"rnn_model_weights.h5\")\n",
    "    return loaded_rnn_model\n",
    "\n",
    "# cnn 모델 로드\n",
    "def get_cnn_model():\n",
    "    loaded_tokenizer=get_tokenizer()\n",
    "    model_name='cnn_classifier_kr'\n",
    "    BATCH_SIZE=16\n",
    "    NUM_EPOCHS=100\n",
    "    VALID_SPLIT=0.1\n",
    "    MAX_LEN=20\n",
    "    word_vocab=loaded_tokenizer.word_index\n",
    "    kargs={'model_name':model_name,\n",
    "          'vocab_size':len(word_vocab)+1,\n",
    "          'embedding_size':64,\n",
    "          'num_filters':100,\n",
    "          'dropout_rate':0.2,\n",
    "          'hidden_dimension':250,\n",
    "          'output_dimension':1}\n",
    "    \n",
    "    loaded_cnn_model=CNNClassifier(**kargs)\n",
    "    loaded_cnn_model.build((75381, 20))\n",
    "    loaded_cnn_model.load_weights(base_path+prediction_path+'cnn_model_weights.h5')\n",
    "    return loaded_cnn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 상품명 전처리 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 함수\n",
    "def preprocessing(title, remove_stopwords=True, stop_words=[]):\n",
    "    hangul = re.compile('[^ ㄱ-ㅣ가-힣]+') # 한글 아닌 문자 제거\n",
    "    title=hangul.sub('', title)\n",
    "    title=' '.join([i for i in title.split(' ') if i!='']) # 여러 공백 하나의 공백으로\n",
    "\n",
    "    if remove_stopwords:\n",
    "        title=[word for word in title.split(' ') if not word in stop_words]\n",
    "        \n",
    "    return title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stop_words 로드 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopwords():\n",
    "    stop_words=[]\n",
    "    with open(\"stop_words.txt\", \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            stop_words.append(line.strip())\n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결과 출력 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_result(product_name, remove_stopwords=True):\n",
    "    \n",
    "    MAX_SEQUENCE_LENGTH=20\n",
    "    \n",
    "    clean_title=[]\n",
    "    \n",
    "    loaded_tokenizer=get_tokenizer()\n",
    "    loaded_rfc_model=get_rfc_model()\n",
    "    loaded_xgb_model=get_xgb_model()\n",
    "    loaded_rnn_model=get_rnn_model()\n",
    "    loaded_cnn_model=get_cnn_model()\n",
    "    stop_words=get_stopwords()\n",
    "    \n",
    "    clean_title.append(preprocessing(product_name, remove_stopwords=remove_stopwords, stop_words=stop_words))\n",
    "    product_sequences=loaded_tokenizer.texts_to_sequences(clean_title)\n",
    "    product_input=pad_sequences(product_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "    \n",
    "    pred_by_rfc=loaded_rfc_model.predict(product_input)\n",
    "    pred_by_xgb=loaded_xgb_model.predict(product_input)\n",
    "    pred_by_rnn=(loaded_rnn_model.predict(product_input) > 0.5).astype(\"int32\")\n",
    "    pred_by_cnn=(loaded_cnn_model.predict(product_input) > 0.5).astype(\"int32\")\n",
    "    \n",
    "    print('rfc 결과: ', pred_by_rfc)\n",
    "    print('xgb 결과: ', pred_by_xgb)\n",
    "    print('rnn 결과: ', pred_by_rnn[0])\n",
    "    print('cnn 결과: ', pred_by_cnn[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dbwls\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.24.0 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "C:\\Users\\dbwls\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.24.0 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rfc 결과:  [0]\n",
      "xgb 결과:  [1]\n",
      "rnn 결과:  [1]\n",
      "cnn 결과:  [1]\n"
     ]
    }
   ],
   "source": [
    "model_result('리베로 기저귀 밴드형 팬티형 구매시 뽀로로 NEW쇼핑카트 증정', remove_stopwords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
